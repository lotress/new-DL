{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNQ12hrbapFx"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Using with Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" apex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivcWodHhapF2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "!cp '/gdrive/My Drive/data.7z' ./\n",
    "!cp '/gdrive/My Drive/address/simhei.ttf' /usr/share/fonts/\n",
    "!7z x data.7z\n",
    "!rm -f data.7z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install ParlAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/ParlAI.git ./ParlAI\n",
    "!python ./ParlAI/setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install Quasi-hyperbolic optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/qhoptim.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pmj1FdGVapF_",
    "outputId": "9cae251b-ecb2-4807-a199-e059d8af021a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting common.py\n"
     ]
    }
   ],
   "source": [
    "%%file common.py\n",
    "import os\n",
    "import argparse\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--rank\", type=int, default=0)\n",
    "args = parser.parse_known_args()[0]\n",
    "def opt():pass\n",
    "if torch.cuda.is_available():\n",
    "  opt.dtype = torch.half\n",
    "  opt.device = torch.device('cuda:{}'.format(args.local_rank))\n",
    "  torch.cuda.set_device(args.local_rank)\n",
    "  opt.cuda = True\n",
    "  from apex import amp\n",
    "else:\n",
    "  opt.device = torch.device('cpu')\n",
    "  opt.dtype = torch.float\n",
    "  opt.cuda = False\n",
    "  num_threads = torch.multiprocessing.cpu_count() - 1\n",
    "  if num_threads > 1:\n",
    "    torch.set_num_threads(num_threads)\n",
    "  amp = None\n",
    "print('Using device ' + str(opt.device))\n",
    "print('Using default dtype ' + str(opt.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Using default dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data.py\n",
    "from common import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataLength = {'train': 4096, 'val': 256, 'test': 256}\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(Data, self).__init__()\n",
    "        l = dataLength[path]\n",
    "        self.lens = torch.randint(4, (l,)) + 1\n",
    "        self.mask = torch.zeros((l, 5), dtype=torch.uint8)\n",
    "        for i in range(l):\n",
    "            self.mask[i, :self.lens[i]].fill_(1)\n",
    "        self.data = torch.rand((l, 5)) * self.mask.float()\n",
    "        self.count = l\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    # input, label, length, mask\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind]\n",
    "        return x, x.sum(), self.lens[ind], self.mask[ind]\n",
    "\n",
    "newLoader = lambda path, *args, **kwargs: DataLoader(Data(path), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vocab.py\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "vocabPath = './char.txt'\n",
    "\n",
    "def getBatch(data):\n",
    "  x = pad_sequence([torch.tensor([1] + [(vocabIndex[t] if t in vocabSet else 0) for t in s] + [0], dtype=torch.long) for s in data])\n",
    "  l = [len(s) + 1 for s in data]\n",
    "  mask = torch.ones_like(x)\n",
    "  for i, t in enumerate(l):\n",
    "    mask[t:, i].fill_(0)\n",
    "  return x, l, mask\n",
    "\n",
    "def initial(path):\n",
    "  global vocab, vocabSet, vocabIndex\n",
    "  with open(path, 'r', encoding='utf-8') as f:\n",
    "    vocab = ['', ''] + f.read().split('\\0')\n",
    "  vocabSet = set(vocab)\n",
    "  vocabIndex = {}\n",
    "  for i, w in enumerate(vocab):\n",
    "    vocabIndex[w] = i\n",
    "  return vocab\n",
    "\n",
    "vocab = []\n",
    "if os.path.exists(vocabPath):\n",
    "  initial(vocabPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjZ72UV0apF8"
   },
   "outputs": [],
   "source": [
    "%%file -a train.py\n",
    "word2vecPath = './vectors.pth'\n",
    "stateDictPath = './net.init.pth'\n",
    "fontPath = '/usr/share/fonts/simhei.ttf'\n",
    "vocabPath = './char.txt'\n",
    "#fontPath = '/usr/share/fonts/wqy-microhei/wqy-microhei.ttc'\n",
    "#fontPath = 'C:/Windows/Fonts/simhei.ttf'\n",
    "from vocab import vocab, initial\n",
    "initial(vocabPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file model.py\n",
    "from common import *\n",
    "\n",
    "Zero = torch.tensor(0.)\n",
    "maxLen = 5\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = opt.device\n",
    "        self.dtype = opt.dtype\n",
    "        self.edim = opt.edim\n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.to(dtype=opt.dtype, device=opt.device)\n",
    "        self.f0 = nn.Linear(1, opt.edim, bias=True)\n",
    "        self.act0 = nn.LeakyReLU(.1)\n",
    "        self.norm = nn.BatchNorm1d(opt.edim * maxLen, affine=False)\n",
    "        self.f1 = nn.Linear(opt.edim * maxLen, 1, bias=True)\n",
    "        self.act1 = torch.tanh\n",
    "\n",
    "    def forward(self, x, mask, *_):\n",
    "        bsz, l = x.shape\n",
    "        e = self.dropout(x).view(bsz, l, 1)\n",
    "        mask = mask.to(e.dtype)\n",
    "        x1 = self.act0(self.f0(e)) * mask.view(bsz, l, 1)\n",
    "        x2 = self.norm(x1.view(bsz, -1)).view(bsz, l, -1) * mask.view(bsz, l, 1)\n",
    "        return self.act1(self.f1(x2.view(bsz, -1)).squeeze(-1)), Zero.to(self.device), x1\n",
    "\n",
    "predict = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dPcJLrxMr9Y"
   },
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbO5FH4NapGB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%file train.py\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from common import *\n",
    "from data import newLoader\n",
    "from model import Model, predict\n",
    "from option import option\n",
    "if amp:\n",
    "    from apex.optimizers import FusedAdam\n",
    "getNelement = lambda model: sum(map(lambda p: p.nelement(), model.parameters()))\n",
    "l1Reg = lambda acc, cur: acc + cur.abs().sum(dtype=torch.float)\n",
    "l2Reg = lambda acc, cur: acc + (cur * cur).sum(dtype=torch.float)\n",
    "nan = torch.tensor(float('nan'), device=opt.device)\n",
    "\n",
    "opt.batchsize = 1\n",
    "opt.epochs = 1\n",
    "opt.maxgrad = 1. # max gradient\n",
    "opt.dropout = 0\n",
    "opt.sdt = 0.001 # initial learning rate\n",
    "opt.sdt_decay_step = 10 # how often to reduce learning rate\n",
    "opt.criterion = lambda y, out, mask: F.mse_loss(out, y) # criterion for evaluation\n",
    "opt.loss = lambda opt, model, y, out, *args: F.mse_loss(out, y) # criterion for loss function\n",
    "opt.newOptimizer = (lambda opt, params, _: FusedAdam(params, lr=opt.sdt)) if amp else lambda opt, params, eps: optim.Adam(params, lr=opt.sdt, amsgrad=True, eps=eps)\n",
    "opt.writer = 0 # TensorBoard writer\n",
    "opt.drawVars = 0\n",
    "opt.reset_parameters = 0\n",
    "opt.__dict__.update(option)\n",
    "\n",
    "def initParameters(opt, model):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'bias') and isinstance(m.bias, torch.Tensor):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        if isinstance(m, nn.PReLU):\n",
    "            nn.init.constant_(next(m.parameters()), 1)\n",
    "        if opt.reset_parameters:\n",
    "            opt.reset_parameters()\n",
    "    if hasattr(model, 'embedding') and isinstance(model.embedding, nn.Embedding):\n",
    "        model.embedding.weight.data[2:] = torch.load(word2vecPath)\n",
    "\n",
    "getParameters = (lambda opt, _: amp.master_params(opt.optimizer)) if amp else lambda _, model: model.parameters()\n",
    "backward = lambda loss, _: loss.backward()\n",
    "if torch.cuda.is_available():\n",
    "    def backward(loss, opt):\n",
    "        with amp.scale_loss(loss, opt.optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "def trainStep(opt, model, x, y, length, mask):\n",
    "    opt.optimizer.zero_grad()\n",
    "    x = x.to(opt.device, non_blocking=True)\n",
    "    mask = mask.to(opt.device, non_blocking=True)\n",
    "    label = y.to(opt.device, dtype=torch.float, non_blocking=True)\n",
    "    loss = opt.loss(opt, model, label, *model(x, mask))\n",
    "    if torch.allclose(loss, nan, equal_nan=True):\n",
    "        raise Exception('Loss returns NaN')\n",
    "    backward(loss, opt)\n",
    "    nn.utils.clip_grad_value_(getParameters(opt, model), opt.maxgrad)\n",
    "    opt.optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "def evaluateStep(opt, model, x, y, _, mask):\n",
    "    mask = mask.to(opt.device, non_blocking=True)\n",
    "    out, *others = model(x.to(opt.device, non_blocking=True), mask)\n",
    "    pred = predict(out)\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        y = y.to(pred)\n",
    "    missed = opt.criterion(y, pred, mask)\n",
    "    return (float(missed.sum()), missed, pred, *others)\n",
    "\n",
    "def evaluate(opt, model):\n",
    "    model.eval()\n",
    "    totalErr = 0\n",
    "    count = 0\n",
    "    for x, y, l, mask in newLoader('val', batch_size=opt.batchsize):\n",
    "        count += int(l.sum())\n",
    "        err, _, pred, _, *others = evaluateStep(opt, model, x, y, l, mask)\n",
    "        totalErr += err\n",
    "    if opt.drawVars:\n",
    "        opt.drawVars(x[0], l[0], *tuple(v[0] for v in others))\n",
    "        print(pred[0])\n",
    "    return totalErr / count\n",
    "\n",
    "def initTrain(opt, model, epoch=None):\n",
    "    opt.optimizer = opt.newOptimizer(opt, model.parameters(), 1e-8)\n",
    "    if opt.sdt_decay_step > 0:\n",
    "        opt.scheduler = optim.lr_scheduler.StepLR(opt.optimizer, opt.sdt_decay_step, gamma=0.5)\n",
    "    else:\n",
    "        opt.scheduler = optim.lr_scheduler.StepLR(opt.optimizer, 1e6, gamma=1)\n",
    "    if type(epoch) == int:\n",
    "        state = torch.load('train.epoch{}.pth'.format(epoch), map_location='cpu')\n",
    "        opt.optimizer.load_state_dict(state[0])\n",
    "        opt.scheduler.load_state_dict(state[1])\n",
    "    else:\n",
    "        torch.manual_seed(args.rank)\n",
    "        np.random.seed(args.rank)\n",
    "\n",
    "def train(opt, model, init=True):\n",
    "    if init:\n",
    "        initParameters(opt, model)\n",
    "        if type(init) == int:\n",
    "            model.load_state_dict(torch.load('model.epoch{}.pth'.format(epoch), map_location='cpu'))\n",
    "    model = model.to(opt.device) # need before constructing optimizers\n",
    "    if init:\n",
    "        initTrain(opt, model, init)\n",
    "    if amp:\n",
    "        model, opt.optimizer = amp.initialize(model, opt.optimizer, opt_level=\"O2\", keep_batchnorm_fp32=False)\n",
    "    for i in range(opt.scheduler.last_epoch, opt.epochs):\n",
    "        opt.scheduler.step()\n",
    "        count = 0\n",
    "        totalLoss = 0\n",
    "        model.train()\n",
    "        opt.optimizer.zero_grad()\n",
    "        for x, y, l, mask in newLoader('train', batch_size=opt.batchsize, shuffle=True):\n",
    "            length = int(l.sum())\n",
    "            count += length\n",
    "            loss = trainStep(opt, model, x, y, length, mask)\n",
    "            totalLoss += loss\n",
    "        valErr = evaluate(opt, model)\n",
    "        if opt.writer:\n",
    "            logBoardStep(opt, model)\n",
    "        print('Epoch #%i | train loss: %.4f | valid error: %.3f | learning rate: %.5f' %\n",
    "          (opt.scheduler.last_epoch, totalLoss / count, valErr, opt.scheduler.get_lr()[0]))\n",
    "        if i % 10 == 9:\n",
    "            saveState(opt, model, opt.scheduler.last_epoch)\n",
    "    return valErr\n",
    "\n",
    "def saveState(opt, model, epoch):\n",
    "    torch.save(model.state_dict(), 'model.epoch{}.pth'.format(epoch))\n",
    "    torch.save((opt.optimizer.state_dict(), opt.scheduler.state_dict()), 'train.epoch{}.pth'.format(epoch))\n",
    "\n",
    "def logBoardStep(opt, model):\n",
    "    step = opt.scheduler.last_epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        try:\n",
    "            opt.writer.add_histogram(name, param.data, step)\n",
    "        except:\n",
    "            print(name, param)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(args.rank)\n",
    "    np.random.seed(args.rank)\n",
    "    model = Model(opt).to(opt.device)\n",
    "    print('Number of parameters: %i | valid error: %.3f' % (getNelement(model), evaluate(opt, model)))\n",
    "    train(opt, model)\n",
    "    torch.save(model.state_dict(), 'model.epoch{}.pth'.format(opt.scheduler.last_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file option.py\n",
    "option = dict(edim=16, epochs=3, maxgrad=1., sdt=1e-2, sdt_decay_step=1, batchsize=32)\n",
    "try:\n",
    "    from qhoptim.pyt import QHAdam\n",
    "    option['newOptimizer'] = lambda opt, params, _: QHAdam(params, lr=opt.sdt, nus=(.7, .8), betas=(0.995, 0.999))\n",
    "except ImportError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_qdhhFzPwUg"
   },
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rcxDyQ52Ap4"
   },
   "outputs": [],
   "source": [
    "%%file -a option.py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "zhfont= mpl.font_manager.FontProperties(fname=fontPath)\n",
    "columns = 2\n",
    "\n",
    "def drawAttention(indices, l, _, att, *args):\n",
    "  if len(att.shape) != 3:\n",
    "    return\n",
    "  heads = att.size(0)\n",
    "  l = int(l)\n",
    "  rows = (heads + columns - 1) // columns\n",
    "  indices = indices[:l].tolist()\n",
    "  ticks = np.arange(0, l)\n",
    "  labels = [''] + [vocab[i] for i in indices]\n",
    "  fig = plt.figure(figsize=(16, rows * 16 // columns))\n",
    "  for t in range(heads):\n",
    "    ax = fig.add_subplot(rows, columns, t + 1)\n",
    "    data = att[t, :l, :l+1].detach().to(torch.float).cpu().numpy()\n",
    "    cax = ax.matshow(data, interpolation='nearest', cmap='hot', vmin=0, vmax=1)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels(labels + ['NA'], fontproperties=zhfont)\n",
    "    ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "  return plt.show()\n",
    "\n",
    "option['drawVars'] = drawAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParlAI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file my_agent.py\n",
    "from common import *\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import numpy as np\n",
    "from parlai.core.torch_agent import TorchAgent, Output\n",
    "from parlai.core.logs import TensorboardLogger\n",
    "from model import Model, predict\n",
    "from train import opt, initParameters, nan\n",
    "\n",
    "class MyAgent(TorchAgent):\n",
    "    def __init__(self, optAgent, shared=None):\n",
    "        super().__init__(optAgent, shared)\n",
    "        if optAgent.get('numthreads', 1) > 1:\n",
    "            torch.set_num_threads(1)\n",
    "        optAgent['gradient_clip'] = opt.maxgrad\n",
    "        self.opt = optAgent\n",
    "        self.criterion = opt.criterion\n",
    "        self.loss = opt.loss\n",
    "        torch.manual_seed(args.rank)\n",
    "        np.random.seed(args.rank)\n",
    "        if optAgent['tensorboard_log'] is True:\n",
    "            self.writer = TensorboardLogger(optAgent)\n",
    "        if not shared:\n",
    "            model = Model(opt)\n",
    "            initParameters(opt, model)\n",
    "            model = model.to(opt.device)\n",
    "            model.train()\n",
    "            self.model = model\n",
    "            if optAgent.get('numthreads', 1) > 1:\n",
    "                model.share_memory()\n",
    "        else:\n",
    "            self.model = share['model']\n",
    "            self.dict = shared['dict']\n",
    "        self.reset()\n",
    "\n",
    "    def share(self):\n",
    "        \"\"\"Share internal states between parent and child instances.\"\"\"\n",
    "        return super().share()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset episode_done.\"\"\"\n",
    "        super().reset()\n",
    "        self.episode_done = True\n",
    "        self.reset_metrics()\n",
    "        return self\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save model, options, dict.\"\"\"\n",
    "        path = self.opt.get('model_file', None) if path is None else path\n",
    "        if not path:\n",
    "            return\n",
    "        states = self.state_dict()\n",
    "        if states:\n",
    "            torch.save(states['model'], path + '.pth')\n",
    "            del states['model']\n",
    "            with open(path + '.states', 'wb') as write:\n",
    "                torch.save(states, write)\n",
    "        # Parlai expects options to also be saved\n",
    "        with open(path + '.opt.json', 'w', encoding='utf-8') as handle:\n",
    "            if hasattr(self, 'model_version'):\n",
    "                self.opt['model_version'] = self.model_version()\n",
    "            saved_opts = deepcopy(self.opt)\n",
    "            if 'interactive_mode' in saved_opts:\n",
    "                # We do not save the state of interactive mode, it is only decided\n",
    "                # by scripts or command line.\n",
    "                del saved_opts['interactive_mode']\n",
    "            json.dump(self.opt, handle)\n",
    "            # for convenience of working with jq, make sure there's a newline\n",
    "            handle.write('\\n')\n",
    "\n",
    "        # force save the dict\n",
    "        self.dict.save(path + '.dict.txt', sort=False)\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Load the state dict into model.\"\"\"\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load model, options, dict.\"\"\"\n",
    "        optPath = path + '.opt.json'\n",
    "        if os.path.isfile(optPath):\n",
    "            with open(optPath, 'r', encoding='utf-8') as handle:\n",
    "                self.opt = json.load(handle)\n",
    "        dictPath = path + '.dict.txt'\n",
    "        if os.path.isfile(dictPath) and hasattr(self, 'dict'):\n",
    "            self.dict.load(dictPath)\n",
    "        statePath = path + '.states'\n",
    "        states = torch.load(statePath, map_location='cpu') if os.path.isfile(statePath) else {}\n",
    "        modelPath = path + '.pth'\n",
    "        if os.path.isfile(modelPath):\n",
    "            states['model'] = torch.load(modelPath, map_location='cpu')\n",
    "            self.load_state_dict(states['model'])\n",
    "        if 'optimizer' in states and hasattr(self, 'optimizer'):\n",
    "            self.optimizer.load_state_dict(states['optimizer'])\n",
    "        return states\n",
    "\n",
    "    def is_valid(self, obs):\n",
    "        \"\"\"Override from TorchAgent.\n",
    "        Check if an observation has no tokens in it.\"\"\"\n",
    "        return len(obs.get('text_vec', [])) > 0\n",
    "\n",
    "    def batchify(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a batch of valid observations from an unchecked batch.\n",
    "\n",
    "        A valid observation is one that passes the lambda provided to the\n",
    "        function, which defaults to checking if the preprocessed 'text_vec'\n",
    "        field is present which would have been set by this agent's 'vectorize'\n",
    "        function.\n",
    "\n",
    "        Returns a namedtuple Batch. See original definition above for in-depth\n",
    "        explanation of each field.\n",
    "\n",
    "        If you want to include additonal fields in the batch, you can subclass\n",
    "        this function and return your own \"Batch\" namedtuple: copy the Batch\n",
    "        namedtuple at the top of this class, and then add whatever additional\n",
    "        fields that you want to be able to access. You can then call\n",
    "        super().batchify(...) to set up the original fields and then set up the\n",
    "        additional fields in your subclass and return that batch instead.\n",
    "\n",
    "        :param obs_batch:\n",
    "            List of vectorized observations\n",
    "\n",
    "        :param sort:\n",
    "            Default False, orders the observations by length of vectors. Set to\n",
    "            true when using torch.nn.utils.rnn.pack_padded_sequence.  Uses the text\n",
    "            vectors if available, otherwise uses the label vectors if available.\n",
    "        \"\"\"\n",
    "        batch = super().batchify(*args, **kwargs)\n",
    "        if not batch.valid_indices or not len(batch.valid_indices):\n",
    "            return batch\n",
    "\n",
    "        lengths = batch.text_lengths\n",
    "        if lengths:\n",
    "            batch.text_lengths = torch.tensor(lengths)\n",
    "            bsz = lengths.shape[0]\n",
    "            text_mask = torch.zeros((bsz, batch.text_vec.shape[1]), dtype=torch.uint8)\n",
    "            for i in range(bsz):\n",
    "                text_mask[i, :lengths[i]].fill_(1)\n",
    "            batch.text_mask = text_mask.cuda() if self.use_cuda else text_mask\n",
    "        return batch\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Process batch of inputs and targets and train on them.\n",
    "\n",
    "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                      version of observations.\n",
    "        \"\"\"\n",
    "        if batch.text_vec is None:\n",
    "            return\n",
    "        self.is_training = True\n",
    "        self.model.train()\n",
    "        self.zero_grad()\n",
    "        output = self.model(batch.text_vec, batch.text_mask)\n",
    "        loss = self.loss(self, self.model, batch.label_vec, *output)\n",
    "        if torch.allclose(loss, nan, equal_nan=True):\n",
    "            raise Exception('Loss returns NaN')\n",
    "        self.backward(loss)\n",
    "        self.update_params()\n",
    "        self.count += int(batch.text_lengths.sum())\n",
    "        self.metrics['loss'] += float(loss)\n",
    "        pred = predict(output[0])\n",
    "        return Output(text=[self.dict.vec2txt(y) for y in pred])\n",
    "\n",
    "    def eval_step(self, batch):\n",
    "        \"\"\"Process batch of inputs.\n",
    "\n",
    "        If the batch includes labels, calculate validation metrics as well.\n",
    "\n",
    "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                      version of observations.\n",
    "        \"\"\"\n",
    "        if batch.text_vec is None:\n",
    "            return\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "        output = self.model(batch.text_vec, batch.text_mask)\n",
    "        if batch.label_vec is not None:\n",
    "            # Interactive mode won't have a gold label\n",
    "            missed = self.criterion(batch.label_vec, pred, batch.text_mask)\n",
    "            self.metrics['error'] += float(missed.sum())\n",
    "            self.eval_exs += batch.label_vec.shape[0]\n",
    "\n",
    "        pred = predict(output[0])\n",
    "        return Output(text=[self.dict.vec2txt(y) for y in pred])\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"Return metrics calculated by the model.\"\"\"\n",
    "        metrics = super().report()\n",
    "        metrics['loss.avg'] = metrics['loss'] / (self.count if self.count else 1)\n",
    "        metrics['error.avg'] = metrics['error'] / (self.eval_exs if self.eval_exs else 1)\n",
    "        return metrics\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset metrics calculated by the model back to zero.\"\"\"\n",
    "        super().reset_metrics()\n",
    "        self.metrics['loss'] = 0.\n",
    "        self.metrics['error'] = 0.\n",
    "        self.count = 0\n",
    "        self.eval_exs = 0\n",
    "\n",
    "    def receive_metrics(self, metrics_dict):\n",
    "        \"\"\"Update lr scheduler with validation loss.\"\"\"\n",
    "        super().receive_metrics(metrics_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def add_cmdline_args(cls, argparser):\n",
    "        \"\"\"Add command-line arguments specifically for this agent.\"\"\"\n",
    "        super(MyAgent, cls).add_cmdline_args(argparser)\n",
    "\n",
    "        agent = argparser.add_argument_group('Arguments')\n",
    "        agent.add_argument('-esz', '--embeddingsize', type=int, default=100,\n",
    "                           help='size of the token embeddings')\n",
    "        agent.add_argument('-dr', '--dropout', type=float, default=0.0,\n",
    "                           help='dropout rate')\n",
    "        agent.add_argument('-rf', '--report-freq', type=float, default=0.001,\n",
    "                           help='Report frequency of prediction during eval.')\n",
    "        agent.add_argument(\n",
    "            '--fp16', type='bool', default=True, help='Use fp16 computations.'\n",
    "        )\n",
    "        agent.add_argument(\n",
    "            '--split-lines',\n",
    "            type='bool',\n",
    "            default=True,\n",
    "            help='split the dialogue history on newlines and save in separate '\n",
    "            'vectors',\n",
    "        )\n",
    "        MyAgent.dictionary_class().add_cmdline_args(argparser)\n",
    "        return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i train.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
