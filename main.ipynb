{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNQ12hrbapFx"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Using with Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" apex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivcWodHhapF2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "!cp '/gdrive/My Drive/data.7z' ./\n",
    "!cp '/gdrive/My Drive/address/simhei.ttf' /usr/share/fonts/\n",
    "!7z x data.7z\n",
    "!rm -f data.7z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install ParlAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/ParlAI.git ./ParlAI\n",
    "!python ./ParlAI/setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install Quasi-hyperbolic optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/qhoptim.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pmj1FdGVapF_",
    "outputId": "9cae251b-ecb2-4807-a199-e059d8af021a"
   },
   "outputs": [],
   "source": [
    "%%file common.py\n",
    "import os\n",
    "import argparse\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--rank\", type=int, default=0)\n",
    "args = parser.parse_known_args()[0]\n",
    "def opt():pass\n",
    "if torch.cuda.is_available():\n",
    "  opt.dtype = torch.half\n",
    "  opt.device = torch.device('cuda:{}'.format(args.local_rank))\n",
    "  torch.cuda.set_device(args.local_rank)\n",
    "  opt.cuda = True\n",
    "  opt.fp16 = True\n",
    "  from apex import amp\n",
    "else:\n",
    "  opt.device = torch.device('cpu')\n",
    "  opt.dtype = torch.float\n",
    "  opt.cuda = False\n",
    "  opt.fp16 = False\n",
    "  num_threads = torch.multiprocessing.cpu_count() - 1\n",
    "  if num_threads > 1:\n",
    "    torch.set_num_threads(num_threads)\n",
    "  amp = None\n",
    "print('Using device ' + str(opt.device))\n",
    "print('Using default dtype ' + str(opt.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data.py\n",
    "from common import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataLength = {'train': 4096, 'val': 256, 'test': 256}\n",
    "vocabsize = 8\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(Data, self).__init__()\n",
    "        l = dataLength[path]\n",
    "        self.lens = torch.ones((l,), dtype=torch.long) * 4\n",
    "        self.mask = torch.ones((l, 4), dtype=torch.uint8)\n",
    "        self.data = torch.randint(vocabsize - 1, (l, 4), dtype=torch.long) + 1\n",
    "        self.count = l\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    # input, label, length, mask\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind]\n",
    "        return x, x, self.lens[ind], self.mask[ind]\n",
    "\n",
    "newLoader = lambda path, *args, **kwargs: DataLoader(Data(path), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vocab.py\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "vocabPath = './char.txt'\n",
    "\n",
    "def getBatch(data):\n",
    "  x = pad_sequence([torch.tensor([1] + [(vocabIndex[t] if t in vocabSet else 0) for t in s] + [0], dtype=torch.long) for s in data])\n",
    "  l = [len(s) + 1 for s in data]\n",
    "  mask = torch.ones_like(x)\n",
    "  for i, t in enumerate(l):\n",
    "    mask[t:, i].fill_(0)\n",
    "  return x, l, mask\n",
    "\n",
    "def initial(path):\n",
    "  global vocab, vocabSet, vocabIndex\n",
    "  with open(path, 'r', encoding='utf-8') as f:\n",
    "    vocab = ['', ''] + f.read().split('\\0')\n",
    "  vocabSet = set(vocab)\n",
    "  vocabIndex = {}\n",
    "  for i, w in enumerate(vocab):\n",
    "    vocabIndex[w] = i\n",
    "  return vocab\n",
    "\n",
    "vocab = []\n",
    "if os.path.exists(vocabPath):\n",
    "  initial(vocabPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjZ72UV0apF8"
   },
   "outputs": [],
   "source": [
    "%%file -a train.py\n",
    "word2vecPath = './vectors.pth'\n",
    "stateDictPath = './net.init.pth'\n",
    "fontPath = '/usr/share/fonts/simhei.ttf'\n",
    "vocabPath = './char.txt'\n",
    "#fontPath = '/usr/share/fonts/wqy-microhei/wqy-microhei.ttc'\n",
    "#fontPath = 'C:/Windows/Fonts/simhei.ttf'\n",
    "from vocab import vocab, initial\n",
    "initial(vocabPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file model.py\n",
    "from common import *\n",
    "\n",
    "Zero = torch.tensor(0.)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = opt.device\n",
    "        self.dtype = opt.dtype\n",
    "        self.edim = opt.edim\n",
    "        self.eps = 1e-4 if opt.fp16 else 1e-8\n",
    "        vocabsize = opt.vocabsize\n",
    "        self.embedding = nn.Embedding(vocabsize, opt.edim)\n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.to(dtype=opt.dtype, device=opt.device)\n",
    "        self.f0 = nn.Linear(opt.edim, opt.edim, bias=True)\n",
    "        self.act0 = nn.LeakyReLU(.1)\n",
    "        self.f1 = nn.Linear(opt.edim, vocabsize, bias=False)\n",
    "\n",
    "    def forward(self, x, mask, *_):\n",
    "        bsz, l = x.shape\n",
    "        e = self.dropout(self.embedding(x))\n",
    "        mask = mask.to(e.dtype).unsqueeze(-1)\n",
    "        x1 = self.act0(self.f0(e)) * mask\n",
    "        x2 = F.normalize(x1, dim=1, eps=self.eps) * mask\n",
    "        return self.f1(x2), Zero.to(self.device), x1\n",
    "\n",
    "with open('model.dict', encoding='utf-8') as fd:\n",
    "    idict = [line.split('\\t')[0] for line in fd if not line.startswith('__FP16_PAD_')]\n",
    "    vocab = len(idict)\n",
    "predict = lambda x, l: [' '.join(idict[i] for i in seq[:l[k]]) for k, seq in enumerate(x[:,:,1:vocab].max(-1)[1] + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dPcJLrxMr9Y"
   },
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbO5FH4NapGB"
   },
   "outputs": [],
   "source": [
    "%%file train.py\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from common import *\n",
    "from data import newLoader\n",
    "from model import Model, predict\n",
    "from option import option\n",
    "if amp:\n",
    "    from apex.optimizers import FusedAdam\n",
    "getNelement = lambda model: sum(map(lambda p: p.nelement(), model.parameters()))\n",
    "l1Reg = lambda acc, cur: acc + cur.abs().sum(dtype=torch.float)\n",
    "l2Reg = lambda acc, cur: acc + (cur * cur).sum(dtype=torch.float)\n",
    "nan = torch.tensor(float('nan'), device=opt.device)\n",
    "\n",
    "opt.batchsize = 1\n",
    "opt.epochs = 1\n",
    "opt.maxgrad = 1. # max gradient\n",
    "opt.dropout = 0\n",
    "opt.sdt = 0.001 # initial learning rate\n",
    "opt.sdt_decay_step = 10 # how often to reduce learning rate\n",
    "opt.criterion = lambda y, out, mask: F.mse_loss(out, y) # criterion for evaluation\n",
    "opt.loss = lambda opt, model, y, out, *args: F.mse_loss(out, y) # criterion for loss function\n",
    "opt.newOptimizer = (lambda opt, params, _: FusedAdam(params, lr=opt.sdt)) if amp else lambda opt, params, eps: optim.Adam(params, lr=opt.sdt, amsgrad=True, eps=eps)\n",
    "opt.writer = 0 # TensorBoard writer\n",
    "opt.drawVars = 0\n",
    "opt.reset_parameters = 0\n",
    "opt.__dict__.update(option)\n",
    "\n",
    "def initParameters(opt, model):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'bias') and isinstance(m.bias, torch.Tensor):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        if isinstance(m, nn.PReLU):\n",
    "            nn.init.constant_(next(m.parameters()), 1)\n",
    "        if opt.reset_parameters:\n",
    "            opt.reset_parameters()\n",
    "#    if hasattr(model, 'embedding') and isinstance(model.embedding, nn.Embedding):\n",
    "#        model.embedding.weight.data[2:] = torch.load(word2vecPath)\n",
    "\n",
    "getParameters = (lambda opt, _: amp.master_params(opt.optimizer)) if amp else lambda _, model: model.parameters()\n",
    "backward = lambda loss, _: loss.backward()\n",
    "if torch.cuda.is_available():\n",
    "    def backward(loss, opt):\n",
    "        with amp.scale_loss(loss, opt.optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "def trainStep(opt, model, x, y, length, mask):\n",
    "    opt.optimizer.zero_grad()\n",
    "    x = x.to(opt.device, non_blocking=True)\n",
    "    mask = mask.to(opt.device, non_blocking=True)\n",
    "    label = y.to(opt.device, non_blocking=True)\n",
    "    loss = opt.loss(opt, model, label, *model(x, mask)).sum()\n",
    "    if torch.allclose(loss, nan, equal_nan=True):\n",
    "        raise Exception('Loss returns NaN')\n",
    "    backward(loss, opt)\n",
    "    nn.utils.clip_grad_value_(getParameters(opt, model), opt.maxgrad)\n",
    "    opt.optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "def evaluateStep(opt, model, x, y, l, mask):\n",
    "    mask = mask.to(opt.device, non_blocking=True)\n",
    "    out, *others = model(x.to(opt.device, non_blocking=True), mask)\n",
    "    pred = predict(out, l)\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        y = y.to(pred)\n",
    "    missed = opt.criterion(y, out, mask)\n",
    "    return (float(missed.sum()), missed, pred, *others)\n",
    "\n",
    "def evaluate(opt, model):\n",
    "    model.eval()\n",
    "    totalErr = 0\n",
    "    count = 0\n",
    "    for x, y, l, mask in newLoader('val', batch_size=opt.batchsize):\n",
    "        count += int(l.sum())\n",
    "        err, _, pred, _, *others = evaluateStep(opt, model, x, y, l, mask)\n",
    "        totalErr += err\n",
    "    if opt.drawVars:\n",
    "        opt.drawVars(x[0], l[0], *tuple(v[0] for v in others))\n",
    "        print(pred[0])\n",
    "    return totalErr / count\n",
    "\n",
    "def initTrain(opt, model, epoch=None):\n",
    "    opt.optimizer = opt.newOptimizer(opt, model.parameters(), 1e-8)\n",
    "    if opt.sdt_decay_step > 0:\n",
    "        opt.scheduler = optim.lr_scheduler.StepLR(opt.optimizer, opt.sdt_decay_step, gamma=0.5)\n",
    "    else:\n",
    "        opt.scheduler = optim.lr_scheduler.StepLR(opt.optimizer, 1e6, gamma=1)\n",
    "    if type(epoch) == int:\n",
    "        state = torch.load('train.epoch{}.pth'.format(epoch), map_location='cpu')\n",
    "        opt.optimizer.load_state_dict(state[0])\n",
    "        opt.scheduler.load_state_dict(state[1])\n",
    "    else:\n",
    "        torch.manual_seed(args.rank)\n",
    "        np.random.seed(args.rank)\n",
    "\n",
    "def train(opt, model, init=True):\n",
    "    if init:\n",
    "        initParameters(opt, model)\n",
    "        if type(init) == int:\n",
    "            model.load_state_dict(torch.load('model.epoch{}.pth'.format(epoch), map_location='cpu'))\n",
    "    model = model.to(opt.device) # need before constructing optimizers\n",
    "    if init:\n",
    "        initTrain(opt, model, init)\n",
    "    if amp:\n",
    "        model, opt.optimizer = amp.initialize(model, opt.optimizer, opt_level=\"O2\", keep_batchnorm_fp32=False)\n",
    "    for i in range(opt.scheduler.last_epoch, opt.epochs):\n",
    "        opt.scheduler.step()\n",
    "        count = 0\n",
    "        totalLoss = 0\n",
    "        model.train()\n",
    "        opt.optimizer.zero_grad()\n",
    "        for x, y, l, mask in newLoader('train', batch_size=opt.batchsize, shuffle=True):\n",
    "            length = int(l.sum())\n",
    "            count += length\n",
    "            loss = trainStep(opt, model, x, y, length, mask)\n",
    "            totalLoss += loss\n",
    "        valErr = evaluate(opt, model)\n",
    "        if opt.writer:\n",
    "            logBoardStep(opt, model)\n",
    "        print('Epoch #%i | train loss: %.4f | valid error: %.3f | learning rate: %.5f' %\n",
    "          (opt.scheduler.last_epoch, totalLoss / count, valErr, opt.scheduler.get_lr()[0]))\n",
    "        if i % 10 == 9:\n",
    "            saveState(opt, model, opt.scheduler.last_epoch)\n",
    "    return valErr\n",
    "\n",
    "def saveState(opt, model, epoch):\n",
    "    torch.save(model.state_dict(), 'model.epoch{}.pth'.format(epoch))\n",
    "    torch.save((opt.optimizer.state_dict(), opt.scheduler.state_dict()), 'train.epoch{}.pth'.format(epoch))\n",
    "\n",
    "def logBoardStep(opt, model):\n",
    "    step = opt.scheduler.last_epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        try:\n",
    "            opt.writer.add_histogram(name, param.data, step)\n",
    "        except:\n",
    "            print(name, param)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(args.rank)\n",
    "    np.random.seed(args.rank)\n",
    "    model = Model(opt).to(opt.device)\n",
    "    print('Number of parameters: %i | valid error: %.3f' % (getNelement(model), evaluate(opt, model)))\n",
    "    train(opt, model)\n",
    "    torch.save(model.state_dict(), 'model.epoch{}.pth'.format(opt.scheduler.last_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file option.py\n",
    "from common import *\n",
    "from data import vocabsize\n",
    "from model import vocab\n",
    "option = dict(edim=16, epochs=3, maxgrad=1., sdt=1e-2, sdt_decay_step=1, batchsize=8, vocabsize=vocabsize)\n",
    "option['loss'] = lambda opt, model, y, out, *_: F.cross_entropy(out.transpose(-1, -2), y, reduction='none')\n",
    "option['criterion'] = lambda y, out, mask, *_: (out[:,:,1:vocab].max(-1)[1] + 1).ne(y).float() * mask.float()\n",
    "try:\n",
    "    from qhoptim.pyt import QHAdam\n",
    "    option['newOptimizer'] = lambda opt, params, _: QHAdam(params, lr=opt.sdt, nus=(.7, .8), betas=(0.995, 0.999))\n",
    "except ImportError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_qdhhFzPwUg"
   },
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rcxDyQ52Ap4"
   },
   "outputs": [],
   "source": [
    "%%file -a option.py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "zhfont= mpl.font_manager.FontProperties(fname=fontPath)\n",
    "columns = 2\n",
    "\n",
    "def drawAttention(indices, l, _, att, *args):\n",
    "  if len(att.shape) != 3:\n",
    "    return\n",
    "  heads = att.size(0)\n",
    "  l = int(l)\n",
    "  rows = (heads + columns - 1) // columns\n",
    "  indices = indices[:l].tolist()\n",
    "  ticks = np.arange(0, l)\n",
    "  labels = [''] + [vocab[i] for i in indices]\n",
    "  fig = plt.figure(figsize=(16, rows * 16 // columns))\n",
    "  for t in range(heads):\n",
    "    ax = fig.add_subplot(rows, columns, t + 1)\n",
    "    data = att[t, :l, :l+1].detach().to(torch.float).cpu().numpy()\n",
    "    cax = ax.matshow(data, interpolation='nearest', cmap='hot', vmin=0, vmax=1)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels(labels + ['NA'], fontproperties=zhfont)\n",
    "    ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "  return plt.show()\n",
    "\n",
    "option['drawVars'] = drawAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParlAI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file my_agent.py\n",
    "from common import *\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import numpy as np\n",
    "from parlai.core.torch_agent import TorchAgent, Output\n",
    "from parlai.core.logs import TensorboardLogger\n",
    "from parlai.core.distributed_utils import is_distributed\n",
    "from model import Model, predict\n",
    "from train import opt, initParameters, logBoardStep, nan\n",
    "\n",
    "def _fixDict(d):\n",
    "    key0 = next(iter(d.keys()))\n",
    "    d.freq[''] = int(key0)\n",
    "    d.ind2tok[0] = ''\n",
    "    d.tok2ind[''] = 0\n",
    "    del d.freq[key0]\n",
    "    del d.tok2ind[key0]\n",
    "    return d\n",
    "\n",
    "class MyAgent(TorchAgent):\n",
    "    def __init__(self, optAgent, shared=None):\n",
    "        init_model, is_finetune = self._get_init_model(optAgent, shared)\n",
    "        super().__init__(optAgent, shared)\n",
    "        if optAgent.get('numthreads', 1) > 1:\n",
    "            torch.set_num_threads(1)\n",
    "        optAgent['gradient_clip'] = opt.maxgrad\n",
    "        self.criterion = opt.criterion\n",
    "        self.loss = opt.loss\n",
    "        self.drawVars = opt.drawVars\n",
    "        opt.edim = optAgent['embeddingsize']\n",
    "        opt.vocabsize = len(self.dict)\n",
    "        opt.__dict__.update(optAgent)\n",
    "        opt.agent = self\n",
    "        torch.manual_seed(args.rank)\n",
    "        np.random.seed(args.rank)\n",
    "        self.writer = TensorboardLogger(optAgent) if optAgent['tensorboard_log'] else 0\n",
    "        if not shared:\n",
    "            model = Model(opt)\n",
    "            self.model = model\n",
    "            if init_model:\n",
    "                print('Loading existing model parameters from ' + init_model)\n",
    "                states = self.load(init_model)\n",
    "            else:\n",
    "                states = {}\n",
    "                initParameters(opt, self.model)\n",
    "            self.model = self.model.to(opt.device)\n",
    "            if self.fp16:\n",
    "                self.model = self.model.half()\n",
    "            self.model.train()\n",
    "            if optAgent.get('numthreads', 1) > 1:\n",
    "                self.model.share_memory()\n",
    "            self.init_optim(model.parameters(), states.get('optimizer'), states.get('saved_optim_type', None))\n",
    "            self.build_lr_scheduler(states, hard_reset=is_finetune)\n",
    "            if is_distributed():\n",
    "                self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[self.opt['gpu']], broadcast_buffers=False)\n",
    "        else:\n",
    "            self.model = shared['model']\n",
    "            self.dict = shared['dict']\n",
    "            if 'optimizer' in shared:\n",
    "                self.optimizer = shared['optimizer']\n",
    "        self.reset()\n",
    "\n",
    "    def _get_init_model(self, opt, shared):\n",
    "        \"\"\"\n",
    "        Get model file to initialize with.\n",
    "\n",
    "        :return:  path to load model from, whether we loaded from `init_model`\n",
    "                  or not\n",
    "        \"\"\"\n",
    "        if shared:\n",
    "            return None, False\n",
    "        if opt.get('model_file') and os.path.isfile(opt['model_file'] + '.opt.json'):\n",
    "            return opt['model_file'], False\n",
    "        if opt.get('init_model') and os.path.isfile(opt['init_model'] + '.opt.json'):\n",
    "            return opt['init_model'], True\n",
    "        return None, False\n",
    "\n",
    "    def build_dictionary(self):\n",
    "        \"\"\"Return the constructed dictionary, which will be set to self.dict.\"\"\"\n",
    "        self.opt.update(dict(dict_nulltoken='', dict_starttoken='', dict_endtoken='', dict_unktoken=''))\n",
    "        d = super().build_dictionary()\n",
    "        if 'dict-file' in self.opt:\n",
    "            d.load(self.opt['dict-file'])\n",
    "        return d # _fixDict(d)\n",
    "\n",
    "    def share(self):\n",
    "        \"\"\"Share internal states between parent and child instances.\"\"\"\n",
    "        shared = super().share()\n",
    "        if hasattr(self, 'optimizer'):\n",
    "            shared['optimizer'] = self.optimizer\n",
    "        return shared\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset episode_done.\"\"\"\n",
    "        super().reset()\n",
    "        self.episode_done = True\n",
    "        return self\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save model, options, dict.\"\"\"\n",
    "        path = self.opt.get('model_file', None) if path is None else path\n",
    "        if not path:\n",
    "            return\n",
    "        states = self.state_dict()\n",
    "        if states:\n",
    "            torch.save(states['model'], path + '.pth')\n",
    "            del states['model']\n",
    "            with open(path + '.states', 'wb') as write:\n",
    "                torch.save(states, write)\n",
    "        # Parlai expects options to also be saved\n",
    "        with open(path + '.opt.json', 'w', encoding='utf-8') as handle:\n",
    "            if hasattr(self, 'model_version'):\n",
    "                self.opt['model_version'] = self.model_version()\n",
    "            saved_opts = deepcopy(self.opt)\n",
    "            if 'interactive_mode' in saved_opts:\n",
    "                # We do not save the state of interactive mode, it is only decided\n",
    "                # by scripts or command line.\n",
    "                del saved_opts['interactive_mode']\n",
    "            json.dump(self.opt, handle)\n",
    "            # for convenience of working with jq, make sure there's a newline\n",
    "            handle.write('\\n')\n",
    "\n",
    "        # force save the dict\n",
    "        dictPath = self.opt['dict-file'] if 'dict-file' in self.opt else path + '.dict.txt'\n",
    "        self.dict.save(dictPath, sort=False)\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Load the state dict into model.\"\"\"\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load model, options, dict.\"\"\"\n",
    "        statePath = path + '.states'\n",
    "        states = torch.load(statePath, map_location='cpu') if os.path.isfile(statePath) else {}\n",
    "        optPath = path + '.opt.json'\n",
    "        if os.path.isfile(optPath):\n",
    "            with open(optPath, 'r', encoding='utf-8') as handle:\n",
    "                self.opt = json.load(handle)\n",
    "                states['saved_optim_type'] = self.opt['optimizer']\n",
    "        modelPath = path + '.pth'\n",
    "        if os.path.isfile(modelPath):\n",
    "            states['model'] = torch.load(modelPath, map_location='cpu')\n",
    "            self.load_state_dict(states['model'])\n",
    "        return states\n",
    "\n",
    "    def is_valid(self, obs):\n",
    "        \"\"\"Override from TorchAgent.\n",
    "        Check if an observation has no tokens in it.\"\"\"\n",
    "        return len(obs.get('text_vec', [])) > 0\n",
    "\n",
    "    def vectorize(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Make vectors out of observation fields and store in the observation.\n",
    "\n",
    "        In particular, the 'text' and 'labels'/'eval_labels' fields are\n",
    "        processed and a new field is added to the observation with the suffix\n",
    "        '_vec'.\n",
    "        \"\"\"\n",
    "        kwargs['add_start'] = False\n",
    "        kwargs['add_end'] = False\n",
    "        return super().vectorize(*args, **kwargs)\n",
    "\n",
    "    def batchify(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a batch of valid observations from an unchecked batch.\n",
    "\n",
    "        A valid observation is one that passes the lambda provided to the\n",
    "        function, which defaults to checking if the preprocessed 'text_vec'\n",
    "        field is present which would have been set by this agent's 'vectorize'\n",
    "        function.\n",
    "\n",
    "        Returns a namedtuple Batch. See original definition above for in-depth\n",
    "        explanation of each field.\n",
    "\n",
    "        If you want to include additonal fields in the batch, you can subclass\n",
    "        this function and return your own \"Batch\" namedtuple: copy the Batch\n",
    "        namedtuple at the top of this class, and then add whatever additional\n",
    "        fields that you want to be able to access. You can then call\n",
    "        super().batchify(...) to set up the original fields and then set up the\n",
    "        additional fields in your subclass and return that batch instead.\n",
    "\n",
    "        :param obs_batch:\n",
    "            List of vectorized observations\n",
    "\n",
    "        :param sort:\n",
    "            Default False, orders the observations by length of vectors. Set to\n",
    "            true when using torch.nn.utils.rnn.pack_padded_sequence.  Uses the text\n",
    "            vectors if available, otherwise uses the label vectors if available.\n",
    "        \"\"\"\n",
    "        batch = super().batchify(*args, **kwargs)\n",
    "        if not batch.valid_indices or not len(batch.valid_indices):\n",
    "            return batch\n",
    "\n",
    "        batch.done_vec = torch.tensor([(1 if ex.get('episode_done') else 0) for ex in batch.observations], dtype=torch.uint8)\n",
    "        lengths = batch.text_lengths\n",
    "        if lengths:\n",
    "            batch.text_lengths = torch.tensor(lengths)\n",
    "            text_mask = torch.zeros(batch.text_vec.shape, dtype=torch.uint8)\n",
    "            for i in range(len(lengths)):\n",
    "                text_mask[i, :lengths[i]].fill_(1)\n",
    "            batch.text_mask = text_mask.cuda() if self.use_cuda else text_mask\n",
    "        return batch\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Process batch of inputs and targets and train on them.\n",
    "\n",
    "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                      version of observations.\n",
    "        \"\"\"\n",
    "        if batch.text_vec is None:\n",
    "            return\n",
    "        self.is_training = True\n",
    "        self.model.train()\n",
    "        self.zero_grad()\n",
    "        output = self.model(batch.text_vec, batch.text_mask)\n",
    "        loss = self.loss(self, self.model, batch.label_vec, *output).sum()\n",
    "        if torch.allclose(loss, nan, equal_nan=True):\n",
    "            raise Exception('Loss returns NaN')\n",
    "        self.backward(loss)\n",
    "        self.update_params()\n",
    "        self.metrics['count'] += int(batch.text_lengths.sum())\n",
    "        self.metrics['loss.sum'] += float(loss)\n",
    "        return # omit response for speed\n",
    "        #pred = predict(output[0], batch.text_lengths)\n",
    "        #return Output(text=pred)\n",
    "\n",
    "    def eval_step(self, batch):\n",
    "        \"\"\"Process batch of inputs.\n",
    "\n",
    "        If the batch includes labels, calculate validation metrics as well.\n",
    "\n",
    "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                      version of observations.\n",
    "        \"\"\"\n",
    "        if batch.text_vec is None:\n",
    "            return\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "        output = self.model(batch.text_vec, batch.text_mask)\n",
    "        if batch.label_vec is not None:\n",
    "            # Interactive mode won't have a gold label\n",
    "            missed = self.criterion(batch.label_vec, output[0], batch.text_mask)\n",
    "            self.metrics['error.sum'] += float(missed.sum())\n",
    "            self.metrics['eval_exs'] += int(batch.text_lengths.sum())\n",
    "\n",
    "        pred = predict(output[0], batch.text_lengths)\n",
    "        text = self._v2t(batch.text_vec[0])\n",
    "        self.metrics['pred'] = (text, pred[0])\n",
    "        self.metrics['vars'] = (batch.text_vec[0], int(batch.text_lengths[0]), *tuple(v[0] for v in output[2:]))\n",
    "        return Output(text=pred)\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"Return metrics calculated by the model.\"\"\"\n",
    "        metrics = super().report()\n",
    "        metrics['loss'] = self.metrics['loss']\n",
    "        metrics['error'] = self.metrics['error.sum'] / (self.metrics['eval_exs'] if self.metrics['eval_exs'] else 1)\n",
    "        metrics['accuracy'] = 1. - metrics['error']\n",
    "        if self.writer:\n",
    "            logBoardStep(self, self.model)\n",
    "        if self.drawVars and 'vars' in self.metrics:\n",
    "            self.drawVars(*self.metrics['vars'])\n",
    "        if 'pred' in self.metrics:\n",
    "            print(self.metrics['pred'])\n",
    "        return metrics\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset metrics calculated by the model back to zero.\"\"\"\n",
    "        if 'loss.sum' in self.metrics:\n",
    "            self.metrics['loss'] = self.metrics['loss.sum'] / (self.metrics['count'] if self.metrics['count'] else 1)\n",
    "        super().reset_metrics()\n",
    "        self.metrics['loss.sum'] = 0.\n",
    "        self.metrics['error.sum'] = 0.\n",
    "        if 'pred' in self.metrics:\n",
    "            del self.metrics['pred']\n",
    "        if 'vars' in self.metrics:\n",
    "            del self.metrics['vars']\n",
    "        self.metrics['count'] = 0\n",
    "        self.metrics['eval_exs'] = 0\n",
    "\n",
    "    def receive_metrics(self, metrics_dict):\n",
    "        \"\"\"Update lr scheduler with validation loss.\"\"\"\n",
    "        return super().receive_metrics(metrics_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def add_cmdline_args(cls, argparser):\n",
    "        \"\"\"Add command-line arguments specifically for this agent.\"\"\"\n",
    "        super(MyAgent, cls).add_cmdline_args(argparser)\n",
    "\n",
    "        agent = argparser.add_argument_group('Arguments')\n",
    "        agent.add_argument('-esz', '--embeddingsize', type=int, default=16,\n",
    "                           help='size of the token embeddings')\n",
    "        agent.add_argument('-dr', '--dropout', type=float, default=0.0,\n",
    "                           help='dropout rate')\n",
    "        argparser.set_defaults(split_lines=True, fp16=True)\n",
    "        MyAgent.dictionary_class().add_cmdline_args(argparser)\n",
    "        return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see one data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.tasks.babi.agents import Task1kTeacher\n",
    "teacher = Task1kTeacher(dict(task='babi:Task1k:0', datapath='/usr/local/anaconda3/lib/python3.6/site-packages/parlai-0.1.0-py3.6.egg/data', datatype='train'))\n",
    "teacher.next_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with ParlAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GIT_PYTHON_GIT_EXECUTABLE'] = '/usr/bin/git'\n",
    "import parlai.scripts.train_model as train\n",
    "from parlai.scripts.train_model import setup_args, TrainLoop\n",
    "from my_agent import MyAgent as ModelClass\n",
    "parser = train.setup_args()\n",
    "parser.add_argument('-f', type=str, default='', help='')\n",
    "ModelClass.add_cmdline_args(parser)\n",
    "opt = parser.parse_args()\n",
    "opt.update({\n",
    "    'model_file': 'model-seq2seq',\n",
    "    'task': 'babi:task1k:0',\n",
    "    'dict_file': 'babi-all.dict',\n",
    "    'dict_lower': True,\n",
    "    'history_size': 1,\n",
    "    'numthreads': 1,\n",
    "    'num_epochs': 10,\n",
    "    'batchsize': 1,\n",
    "    'optimizer': 'sgd',\n",
    "    'learningrate': .05,\n",
    "    'momentum': .9,\n",
    "    'save_after_valid': True,\n",
    "    'validation_every_n_epochs': 1,\n",
    "    'metrics': 'correct,accuracy',\n",
    "    'log_every_n_secs': 30\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.create_agent = lambda opt, *_: ModelClass(opt)\n",
    "trainLoop = train.TrainLoop(opt)\n",
    "res = trainLoop.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
