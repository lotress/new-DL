{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNQ12hrbapFx"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Using with Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" apex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivcWodHhapF2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "!cp '/gdrive/My Drive/data.7z' ./\n",
    "!cp '/gdrive/My Drive/address/simhei.ttf' /usr/share/fonts/\n",
    "!7z x data.7z\n",
    "!rm -f data.7z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install ParlAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lotress/ParlAI.git ./ParlAI\n",
    "!python ./ParlAI/setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install Quasi-hyperbolic optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/qhoptim.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pmj1FdGVapF_",
    "outputId": "9cae251b-ecb2-4807-a199-e059d8af021a"
   },
   "outputs": [],
   "source": [
    "%%file common.py\n",
    "import os\n",
    "import argparse\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--rank\", type=int, default=0)\n",
    "args = parser.parse_known_args()[0]\n",
    "def opt():pass\n",
    "if torch.cuda.is_available():\n",
    "  opt.dtype = torch.half\n",
    "  opt.device = torch.device('cuda:{}'.format(args.local_rank))\n",
    "  torch.cuda.set_device(args.local_rank)\n",
    "  opt.cuda = True\n",
    "  opt.fp16 = 2\n",
    "  from apex import amp\n",
    "else:\n",
    "  opt.device = torch.device('cpu')\n",
    "  opt.dtype = torch.float\n",
    "  opt.cuda = False\n",
    "  opt.fp16 = 0\n",
    "  num_threads = torch.multiprocessing.cpu_count() - 1\n",
    "  if num_threads > 1:\n",
    "    torch.set_num_threads(num_threads)\n",
    "  amp = None\n",
    "print('Using device ' + str(opt.device))\n",
    "print('Using default dtype ' + str(opt.dtype))\n",
    "\n",
    "upTruncBy8 = lambda x: (-x & -8 ^ -1) + 1\n",
    "\n",
    "def getWriter(name='.', writer=None):\n",
    "    import torchvision.utils as vutils\n",
    "    if not writer:\n",
    "        from tensorboardX import SummaryWriter\n",
    "        writer = SummaryWriter(name)\n",
    "    def write(scalars={}, images={}, histograms={}, n=0):\n",
    "        for key in scalars:\n",
    "            writer.add_scalar(key, scalars[key], n)\n",
    "        for key in images:\n",
    "            x = vutils.make_grid(images[key])\n",
    "            writer.add_image(key, x, n)\n",
    "        for key in histograms:\n",
    "            writer.add_histogram(key, histograms[key].data, n)\n",
    "    return write, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data.py\n",
    "from common import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataLength = {'train': 4096, 'val': 256, 'test': 256}\n",
    "vocabsize = 8\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(Data, self).__init__()\n",
    "        l = dataLength[path]\n",
    "        self.lens = torch.ones((l,), dtype=torch.long) * 4\n",
    "        self.mask = torch.ones((l, 4), dtype=torch.uint8)\n",
    "        self.data = torch.randint(vocabsize - 1, (l, 4), dtype=torch.long) + 1\n",
    "        self.count = l\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    # input, label, length, mask\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind]\n",
    "        return x, x, self.lens[ind], self.mask[ind]\n",
    "\n",
    "newLoader = lambda path, *args, **kwargs: DataLoader(Data(path), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vocab.py\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "vocabPath = './char.txt'\n",
    "\n",
    "def getBatch(data):\n",
    "  x = pad_sequence([torch.tensor([1] + [(vocabIndex[t] if t in vocabSet else 0) for t in s] + [0], dtype=torch.long) for s in data])\n",
    "  l = [len(s) + 1 for s in data]\n",
    "  mask = torch.ones_like(x)\n",
    "  for i, t in enumerate(l):\n",
    "    mask[t:, i].fill_(0)\n",
    "  return x, l, mask\n",
    "\n",
    "def initial(path):\n",
    "  global vocab, vocabSet, vocabIndex\n",
    "  with open(path, 'r', encoding='utf-8') as f:\n",
    "    vocab = ['', ''] + f.read().split('\\0')\n",
    "  vocabSet = set(vocab)\n",
    "  vocabIndex = {}\n",
    "  for i, w in enumerate(vocab):\n",
    "    vocabIndex[w] = i\n",
    "  return vocab\n",
    "\n",
    "vocab = []\n",
    "if os.path.exists(vocabPath):\n",
    "  initial(vocabPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjZ72UV0apF8"
   },
   "outputs": [],
   "source": [
    "%%file -a train.py\n",
    "word2vecPath = './vectors.pth'\n",
    "stateDictPath = './net.init.pth'\n",
    "fontPath = '/usr/share/fonts/simhei.ttf'\n",
    "vocabPath = './char.txt'\n",
    "#fontPath = '/usr/share/fonts/wqy-microhei/wqy-microhei.ttc'\n",
    "#fontPath = 'C:/Windows/Fonts/simhei.ttf'\n",
    "from vocab import vocab, initial\n",
    "initial(vocabPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file model.py\n",
    "from common import *\n",
    "\n",
    "Zero = torch.tensor(0.)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = opt.device\n",
    "        self.dtype = opt.dtype\n",
    "        self.edim = opt.edim\n",
    "        self.eps = 1e-4 if opt.fp16 else 1e-8\n",
    "        vocabsize = opt.vocabsize\n",
    "        self.embedding = nn.Embedding(vocabsize, opt.edim)\n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.to(dtype=opt.dtype, device=opt.device)\n",
    "        self.f0 = nn.Linear(opt.edim, opt.edim, bias=True)\n",
    "        self.act0 = nn.LeakyReLU(.1)\n",
    "        self.f1 = nn.Linear(opt.edim, vocabsize, bias=False)\n",
    "\n",
    "    def forward(self, x, mask, *_):\n",
    "        bsz, l = x.shape\n",
    "        e = self.dropout(self.embedding(x))\n",
    "        mask = mask.to(e.dtype).unsqueeze(-1)\n",
    "        x1 = self.act0(self.f0(e)) * mask\n",
    "        x2 = F.normalize(x1, dim=1, eps=self.eps) * mask\n",
    "        return self.f1(x2), Zero.to(self.device), x1\n",
    "\n",
    "with open('integrationTests.dict', encoding='utf-8') as fd:\n",
    "    idict = [line.split('\\t')[0] for line in fd if not line.startswith('__FP16_PAD_')]\n",
    "    vocab = len(idict)\n",
    "predict = lambda x, l: [' '.join(idict[i] for i in seq[:l[k]]) for k, seq in enumerate(x[:,:,1:vocab].max(-1)[1] + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dPcJLrxMr9Y"
   },
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbO5FH4NapGB"
   },
   "outputs": [],
   "source": [
    "%%file train.py\n",
    "from common import *\n",
    "import torch.optim as optim\n",
    "from data import newLoader\n",
    "from model import Model, predict\n",
    "from option import option\n",
    "if amp:\n",
    "  from apex.optimizers import FusedAdam\n",
    "getNelement = lambda model: sum(map(lambda p: p.nelement(), model.parameters()))\n",
    "l1Reg = lambda acc, cur: acc + cur.abs().sum(dtype=torch.float)\n",
    "l2Reg = lambda acc, cur: acc + (cur * cur).sum(dtype=torch.float)\n",
    "nan = torch.tensor(float('nan'), device=opt.device)\n",
    "toDevice = lambda a, device: tuple(map(lambda x: x.to(device, non_blocking=True) if isinstance(x, torch.Tensor) else x, a))\n",
    "detach0 = lambda x: x[0].detach() if isinstance(x, torch.Tensor) else x[0]\n",
    "\n",
    "def initParameters(opt, model):\n",
    "  for m in model.modules():\n",
    "    if hasattr(m, 'bias') and isinstance(m.bias, torch.Tensor):\n",
    "      nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.PReLU):\n",
    "      nn.init.constant_(next(m.parameters()), 1)\n",
    "    if hasattr(m, '_reset_parameters'):\n",
    "      m._reset_parameters()\n",
    "  if opt.reset_parameters:\n",
    "    opt.reset_parameters(opt, model)\n",
    "\n",
    "def getParamOptions(opt, model, *config):\n",
    "  res = []\n",
    "  s = set()\n",
    "  base = opt.learningrate\n",
    "  for m, k in config:\n",
    "    s = s.union(set(m.parameters()))\n",
    "    res.append(dict(params=m.parameters(), lr=k * base))\n",
    "  res.append({'params': filter(lambda p: not p in s, model.parameters())})\n",
    "  return res\n",
    "\n",
    "def trainStep(opt, model, x, y, length, *args):\n",
    "  optimizer = opt.optimizer\n",
    "  optimizer.zero_grad()\n",
    "  d = opt.device\n",
    "  loss = opt.loss(opt, model, y.to(d), *model(x.to(d), *toDevice(args, d))).sum()\n",
    "  if torch.allclose(loss, nan, equal_nan=True):\n",
    "    raise Exception('Loss returns NaN')\n",
    "  backward(loss, opt)\n",
    "  if hasattr(opt, 'gradF'):\n",
    "    opt.gradF(model, getParameters(opt, model))\n",
    "  nn.utils.clip_grad_value_(getParameters(opt, model), opt.maxgrad)\n",
    "  opt.optimizer.step()\n",
    "  if hasattr(opt, 'paraF'):\n",
    "    opt.paraF(opt, model)\n",
    "  return float(loss) * float(length)\n",
    "\n",
    "def evaluateStep(opt, model, x, y, l, *args):\n",
    "  args = toDevice(args, opt.device)\n",
    "  out, *others = model(x.to(opt.device, non_blocking=True), *args)\n",
    "  pred = predict(out, l)\n",
    "  if isinstance(pred, torch.Tensor):\n",
    "    y = y.to(pred)\n",
    "  missed = opt.criterion(y, out, *args)\n",
    "  return (float(missed.sum()), missed, pred, *others)\n",
    "\n",
    "def evaluate(opt, model, path='val'):\n",
    "  model.eval()\n",
    "  totalErr = 0\n",
    "  count = 0\n",
    "  for x, y, l, *args in newLoader(path, batch_size=opt.batchsize):\n",
    "    count += int(l.sum())\n",
    "    err, _, pred, _, *others = evaluateStep(opt, model, x, y, l, *args)\n",
    "    totalErr += err\n",
    "  vs = tuple(map(detach0, others))\n",
    "  if opt.drawVars:\n",
    "    opt.drawVars(x[0], l[0], *vs)\n",
    "    print(pred[0])\n",
    "  return totalErr / count, opt.toImages(*vs) if opt.toImages else {}\n",
    "\n",
    "def initTrain(opt, model, epoch=None):\n",
    "  paramOptions = getParamOptions(opt, model)\n",
    "  eps = 1e-4 if opt.fp16 else 1e-8\n",
    "  opt.optimizer = opt.newOptimizer(opt, paramOptions, eps)\n",
    "  if opt.sdt_decay_step > 0:\n",
    "    gamma = opt.gamma if hasattr(opt, 'gamma') else .5\n",
    "    opt.scheduler = optim.lr_scheduler.StepLR(opt.optimizer, opt.sdt_decay_step, gamma=gamma)\n",
    "  else:\n",
    "    opt.scheduler = None\n",
    "  if type(epoch) == int:\n",
    "    state = torch.load('train.epoch{}.pth'.format(epoch), map_location='cpu')\n",
    "    opt.optimizer.load_state_dict(state[0])\n",
    "    opt.scheduler.load_state_dict(state[1])\n",
    "  else:\n",
    "    torch.manual_seed(args.rank)\n",
    "    np.random.seed(args.rank)\n",
    "\n",
    "def train(opt, model, init=True):\n",
    "  if init:\n",
    "    initParameters(opt, model)\n",
    "    if type(init) == int:\n",
    "      model.load_state_dict(torch.load('model.epoch{}.pth'.format(init), map_location='cpu'))\n",
    "  model = model.to(opt.device) # need before constructing optimizers\n",
    "  if init:\n",
    "    initTrain(opt, model, init)\n",
    "  if opt.cuda and opt.fp16:\n",
    "    model, opt.optimizer = amp.initialize(model, opt.optimizer, opt_level=\"O{}\".format(opt.fp16))\n",
    "  for i in range(opt.scheduler.last_epoch, opt.epochs):\n",
    "    count = 0\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for x, y, l, *args in newLoader('train', batch_size=opt.batchsize, shuffle=True):\n",
    "      length = int(l.sum())\n",
    "      count += length\n",
    "      loss = trainStep(opt, model, x, y, length, *args)\n",
    "      totalLoss += loss\n",
    "    if opt.scheduler:\n",
    "      opt.scheduler.step()\n",
    "    valErr, vs = evaluate(opt, model)\n",
    "    avgLoss = totalLoss / count\n",
    "    if opt.writer:\n",
    "      opt.writer({'loss': avgLoss}, images=vs, histograms=dict(model.named_parameters()), n=opt.scheduler.last_epoch)\n",
    "    print('Epoch #%i | train loss: %.4f | valid error: %.4f | learning rate: %.5f' %\n",
    "      (opt.scheduler.last_epoch, avgLoss, valErr, opt.scheduler.get_lr()[0]))\n",
    "    if i % 10 == 9:\n",
    "      saveState(opt, model, opt.scheduler.last_epoch)\n",
    "  return valErr\n",
    "\n",
    "def saveState(opt, model, epoch):\n",
    "  torch.save(model.state_dict(), 'model.epoch{}.pth'.format(epoch))\n",
    "  torch.save((opt.optimizer.state_dict(), opt.scheduler.state_dict()), 'train.epoch{}.pth'.format(epoch))\n",
    "\n",
    "try:\n",
    "  from data import init\n",
    "  init()\n",
    "except ImportError: pass\n",
    "opt.batchsize = 1\n",
    "opt.epochs = 1\n",
    "opt.maxgrad = 1. # max gradient\n",
    "opt.dropout = 0\n",
    "opt.learningrate = 0.001 # initial learning rate\n",
    "opt.sdt_decay_step = 10 # how often to reduce learning rate\n",
    "opt.criterion = lambda y, out, mask, *args: F.mse_loss(out, y) # criterion for evaluation\n",
    "opt.loss = lambda opt, model, y, out, *args: F.mse_loss(out, y) # criterion for loss function\n",
    "opt.newOptimizer = (lambda opt, params, _: FusedAdam(params, lr=opt.learningrate)) if amp else lambda opt, params, eps: optim.Adam(params, lr=opt.learningrate, amsgrad=True, eps=eps)\n",
    "opt.writer = 0 # TensorBoard writer\n",
    "opt.drawVars = 0\n",
    "opt.reset_parameters = 0\n",
    "opt.toImages = 0\n",
    "opt.__dict__.update(option)\n",
    "if opt.cuda and opt.fp16 > 1:\n",
    "  getParameters = lambda opt, _: amp.master_params(opt.optimizer)\n",
    "  def backward(loss, opt):\n",
    "    with amp.scale_loss(loss, opt.optimizer) as scaled_loss:\n",
    "      scaled_loss.backward()\n",
    "else:\n",
    "  getParameters = lambda _, model: model.parameters()\n",
    "  backward = lambda loss, _: loss.backward()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  torch.manual_seed(args.rank)\n",
    "  np.random.seed(args.rank)\n",
    "  model = Model(opt).to(opt.device)\n",
    "  print('Number of parameters: %i | valid error: %.3f' % (getNelement(model), evaluate(opt, model)[0]))\n",
    "  train(opt, model)\n",
    "  modelName = 'model.epoch{}.pth'.format(opt.scheduler.last_epoch) if hasattr(opt, 'scheduler') else 'model.pth'\n",
    "  torch.save(model.state_dict(), modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file option.py\n",
    "from common import *\n",
    "from model import vocab\n",
    "option = dict(edim=16, epochs=3, maxgrad=1., sdt=1e-2, sdt_decay_step=1, batchsize=8, vocabsize=vocab, fp16=2)\n",
    "option['loss'] = lambda opt, model, y, out, *_: F.cross_entropy(out.transpose(-1, -2), y, reduction='none')\n",
    "option['criterion'] = lambda y, out, mask, *_: (out[:,:,1:vocab].max(-1)[1] + 1).ne(y).float() * mask.float()\n",
    "try:\n",
    "    from qhoptim.pyt import QHAdam\n",
    "    option['newOptimizer'] = lambda opt, params, _: QHAdam(params, lr=opt.learningrate, nus=(.7, .8), betas=(0.995, 0.999))\n",
    "except ImportError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_qdhhFzPwUg"
   },
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rcxDyQ52Ap4"
   },
   "outputs": [],
   "source": [
    "%%file -a option.py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "zhfont= mpl.font_manager.FontProperties(fname=fontPath)\n",
    "columns = 2\n",
    "\n",
    "def drawAttention(indices, l, _, att, *args):\n",
    "  if len(att.shape) != 3:\n",
    "    return\n",
    "  heads = att.size(0)\n",
    "  l = int(l)\n",
    "  rows = (heads + columns - 1) // columns\n",
    "  indices = indices[:l].tolist()\n",
    "  ticks = np.arange(0, l)\n",
    "  labels = [''] + [vocab[i] for i in indices]\n",
    "  fig = plt.figure(figsize=(16, rows * 16 // columns))\n",
    "  for t in range(heads):\n",
    "    ax = fig.add_subplot(rows, columns, t + 1)\n",
    "    data = att[t, :l, :l+1].detach().to(torch.float).cpu().numpy()\n",
    "    cax = ax.matshow(data, interpolation='nearest', cmap='hot', vmin=0, vmax=1)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels(labels + ['NA'], fontproperties=zhfont)\n",
    "    ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "  return plt.show()\n",
    "\n",
    "option['drawVars'] = drawAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParlAI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file my_agent.py\n",
    "from common import *\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import numpy as np\n",
    "from parlai.core.torch_agent import TorchAgent, Output\n",
    "from parlai.core.logs import TensorboardLogger\n",
    "from parlai.utils.distributed import is_distributed\n",
    "from model import Model, predict\n",
    "from train import opt, initParameters, getParamOptions, nan\n",
    "\n",
    "def _isFp16(x):\n",
    "  s = x.lower()\n",
    "  return 2 if s == 'true' else 0 if s == 'false' else int(x)\n",
    "\n",
    "class MyAgent(TorchAgent):\n",
    "  def __init__(self, optAgent, shared=None):\n",
    "    init_model, is_finetune = self._get_init_model(optAgent, shared)\n",
    "    super().__init__(optAgent, shared)\n",
    "    if optAgent.get('numthreads', 1) > 1:\n",
    "      torch.set_num_threads(1)\n",
    "    optAgent['gradient_clip'] = opt.maxgrad\n",
    "    self.criterion = opt.criterion\n",
    "    self.loss = opt.loss\n",
    "    self.drawVars = opt.drawVars\n",
    "    opt.edim = optAgent['embeddingsize']\n",
    "    opt.vocabsize = len(self.dict)\n",
    "    opt.__dict__.update(optAgent)\n",
    "    opt.agent = self\n",
    "    opt.fp16 = self.fp16\n",
    "    torch.manual_seed(args.rank)\n",
    "    np.random.seed(args.rank)\n",
    "    self.writeVars = 0\n",
    "    self.vars = {}\n",
    "    if optAgent['tensorboard_log']:\n",
    "      self.writeVars, *_ = getWriter(writer=TensorboardLogger(optAgent))\n",
    "    if self.fp16:\n",
    "      try:\n",
    "        from apex import amp\n",
    "      except ImportError:\n",
    "        raise ImportError(\n",
    "          'No fp16 support without apex. Please install it from '\n",
    "          'https://github.com/NVIDIA/apex'\n",
    "        )\n",
    "      self.getParameters = lambda: amp.master_params(self.optimizer)\n",
    "      self.amp = amp\n",
    "    else:\n",
    "      self.getParameters = lambda: self.model.parameters()\n",
    "    if not shared:\n",
    "      model = Model(opt)\n",
    "      self.model = model\n",
    "      if init_model:\n",
    "        print('Loading existing model parameters from ' + init_model)\n",
    "        states = self.load(init_model)\n",
    "      else:\n",
    "        states = {}\n",
    "        initParameters(opt, self.model)\n",
    "      if self.use_cuda:\n",
    "        self.model.cuda()\n",
    "      self.model.train()\n",
    "      if optAgent.get('numthreads', 1) > 1:\n",
    "        self.model.share_memory()\n",
    "      paramOptions = getParamOptions(opt, self.model)\n",
    "      self.init_optim(paramOptions, states.get('optimizer'), states.get('saved_optim_type', None))\n",
    "      self.build_lr_scheduler(states, hard_reset=is_finetune)\n",
    "      if is_distributed():\n",
    "        self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[self.opt['gpu']], broadcast_buffers=False)\n",
    "      self.reset()\n",
    "    else:\n",
    "      self.model = shared['model']\n",
    "      self.dict = shared['dict']\n",
    "      if 'optimizer' in shared:\n",
    "        self.optimizer = shared['optimizer']\n",
    "\n",
    "  def _get_init_model(self, opt, shared):\n",
    "    \"\"\"\n",
    "    Get model file to initialize with.\n",
    "\n",
    "    :return:  path to load model from, whether we loaded from `init_model`\n",
    "              or not\n",
    "    \"\"\"\n",
    "    if shared:\n",
    "      return None, False\n",
    "    if opt.get('model_file') and os.path.isfile(opt['model_file'] + '.opt.json'):\n",
    "      return opt['model_file'], False\n",
    "    if opt.get('init_model') and os.path.isfile(opt['init_model'] + '.opt.json'):\n",
    "      return opt['init_model'], True\n",
    "    return None, False\n",
    "\n",
    "  def build_dictionary(self):\n",
    "    \"\"\"Return the constructed dictionary, which will be set to self.dict.\"\"\"\n",
    "    d = super().build_dictionary()\n",
    "    if 'dict_file' in self.opt:\n",
    "      d.load(self.opt['dict_file'])\n",
    "    return d\n",
    "\n",
    "  def share(self):\n",
    "    \"\"\"Share internal states between parent and child instances.\"\"\"\n",
    "    shared = super().share()\n",
    "    if hasattr(self, 'optimizer'):\n",
    "      shared['optimizer'] = self.optimizer\n",
    "    return shared\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Reset episode_done.\"\"\"\n",
    "    super().reset()\n",
    "    self.episode_done = True\n",
    "    return self\n",
    "\n",
    "  def save(self, path):\n",
    "    \"\"\"Save model, options, dict.\"\"\"\n",
    "    path = self.opt.get('model_file', None) if path is None else path\n",
    "    if not path:\n",
    "      return\n",
    "    states = self.state_dict()\n",
    "    if states:\n",
    "      torch.save(states['model'], path + '.pth')\n",
    "      del states['model']\n",
    "      with open(path + '.states', 'wb') as write:\n",
    "        torch.save(states, write)\n",
    "    # Parlai expects options to also be saved\n",
    "    with open(path + '.opt.json', 'w', encoding='utf-8') as handle:\n",
    "      if hasattr(self, 'model_version'):\n",
    "        self.opt['model_version'] = self.model_version()\n",
    "      saved_opts = deepcopy(self.opt)\n",
    "      if 'interactive_mode' in saved_opts:\n",
    "        # We do not save the state of interactive mode, it is only decided\n",
    "        # by scripts or command line.\n",
    "        del saved_opts['interactive_mode']\n",
    "      json.dump(self.opt, handle)\n",
    "      # for convenience of working with jq, make sure there's a newline\n",
    "      handle.write('\\n')\n",
    "\n",
    "    # force save the dict\n",
    "    dictPath = self.opt['dict_file'] if 'dict_file' in self.opt else path + '.dict.txt'\n",
    "    self.dict.save(dictPath, sort=False)\n",
    "\n",
    "  def load_state_dict(self, state_dict):\n",
    "    \"\"\"Load the state dict into model.\"\"\"\n",
    "    self.model.load_state_dict(state_dict)\n",
    "    if self.use_cuda:\n",
    "      self.model.cuda()\n",
    "\n",
    "  def load(self, path):\n",
    "    \"\"\"Load model, options, dict.\"\"\"\n",
    "    statePath = path + '.states'\n",
    "    states = torch.load(statePath, map_location='cpu') if os.path.isfile(statePath) else {}\n",
    "    optPath = path + '.opt.json'\n",
    "    if os.path.isfile(optPath):\n",
    "      with open(optPath, 'r', encoding='utf-8') as handle:\n",
    "        self.opt = json.load(handle)\n",
    "        states['saved_optim_type'] = self.opt['optimizer']\n",
    "    modelPath = path + '.pth'\n",
    "    if os.path.isfile(modelPath):\n",
    "      states['model'] = torch.load(modelPath, map_location='cpu')\n",
    "      self.load_state_dict(states['model'])\n",
    "    return states\n",
    "\n",
    "  def is_valid(self, obs):\n",
    "    \"\"\"Override from TorchAgent.\n",
    "    Check if an observation has no tokens in it.\"\"\"\n",
    "    return len(obs.get('text_vec', [])) > 0\n",
    "\n",
    "  def vectorize(self, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Make vectors out of observation fields and store in the observation.\n",
    "\n",
    "    In particular, the 'text' and 'labels'/'eval_labels' fields are\n",
    "    processed and a new field is added to the observation with the suffix\n",
    "    '_vec'.\n",
    "    \"\"\"\n",
    "    kwargs['add_start'] = False\n",
    "    kwargs['add_end'] = False\n",
    "    return super().vectorize(*args, **kwargs)\n",
    "\n",
    "  def batchify(self, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a batch of valid observations from an unchecked batch.\n",
    "\n",
    "    A valid observation is one that passes the lambda provided to the\n",
    "    function, which defaults to checking if the preprocessed 'text_vec'\n",
    "    field is present which would have been set by this agent's 'vectorize'\n",
    "    function.\n",
    "\n",
    "    Returns a namedtuple Batch. See original definition above for in-depth\n",
    "    explanation of each field.\n",
    "\n",
    "    If you want to include additonal fields in the batch, you can subclass\n",
    "    this function and return your own \"Batch\" namedtuple: copy the Batch\n",
    "    namedtuple at the top of this class, and then add whatever additional\n",
    "    fields that you want to be able to access. You can then call\n",
    "    super().batchify(...) to set up the original fields and then set up the\n",
    "    additional fields in your subclass and return that batch instead.\n",
    "\n",
    "    :param obs_batch:\n",
    "        List of vectorized observations\n",
    "\n",
    "    :param sort:\n",
    "        Default False, orders the observations by length of vectors. Set to\n",
    "        true when using torch.nn.utils.rnn.pack_padded_sequence.  Uses the text\n",
    "        vectors if available, otherwise uses the label vectors if available.\n",
    "    \"\"\"\n",
    "    batch = super().batchify(*args, **kwargs)\n",
    "    if not batch.valid_indices or not len(batch.valid_indices):\n",
    "      return batch\n",
    "\n",
    "    batch.done_vec = torch.tensor([(1 if ex.get('episode_done') else 0) for ex in batch.observations], dtype=torch.uint8)\n",
    "    lengths = batch.text_lengths\n",
    "    if lengths:\n",
    "      batch.text_lengths = torch.tensor(lengths)\n",
    "      text_mask = torch.zeros(batch.text_vec.shape, dtype=torch.uint8)\n",
    "      for i in range(len(lengths)):\n",
    "        text_mask[i, :lengths[i]].fill_(1)\n",
    "      batch.text_mask = text_mask.cuda() if self.use_cuda else text_mask\n",
    "    return batch\n",
    "\n",
    "  def init_optim(self, params, optim_states=None, saved_optim_type=None):\n",
    "    \"\"\"\n",
    "    Initialize optimizer with model parameters.\n",
    "    \"\"\"\n",
    "    opt = self.opt\n",
    "\n",
    "    # set up optimizer args\n",
    "    lr = opt['learningrate']\n",
    "    kwargs = {'lr': lr}\n",
    "    if opt.get('weight_decay'):\n",
    "      kwargs['weight_decay'] = opt['weight_decay']\n",
    "    if opt.get('momentum') > 0 and opt['optimizer'] in ['sgd', 'rmsprop', 'qhm']:\n",
    "      # turn on momentum for optimizers that use it\n",
    "      kwargs['momentum'] = opt['momentum']\n",
    "      if opt['optimizer'] == 'sgd' and opt.get('nesterov', True):\n",
    "        # for sgd, maybe nesterov\n",
    "        kwargs['nesterov'] = opt.get('nesterov', True)\n",
    "      elif opt['optimizer'] == 'qhm':\n",
    "        # qhm needs a nu\n",
    "        kwargs['nu'] = opt.get('nus', (0.7,))[0]\n",
    "    elif opt['optimizer'] == 'adam':\n",
    "      # turn on amsgrad for adam\n",
    "      # amsgrad paper: https://openreview.net/forum?id=ryQu7f-RZ\n",
    "      kwargs['amsgrad'] = True\n",
    "    elif opt['optimizer'] == 'qhadam':\n",
    "      # set nus for qhadam\n",
    "      kwargs['nus'] = opt.get('nus', (0.7, 1.0))\n",
    "    if opt['optimizer'] in ['adam', 'sparseadam', 'fused_adam', 'adamax', 'qhadam']:\n",
    "      # set betas for optims that use it\n",
    "      kwargs['betas'] = opt.get('betas', (0.9, 0.999))\n",
    "      # set adam optimizer, but only if user specified it\n",
    "      if opt.get('adam_eps'):\n",
    "        kwargs['eps'] = opt['adam_eps']\n",
    "\n",
    "    optim_class = self.optim_opts()[opt['optimizer']]\n",
    "    self.optimizer = optim_class(params, **kwargs)\n",
    "    if self.fp16:\n",
    "      self.model, self.optimizer = self.amp.initialize(self.model, self.optimizer, opt_level=\"O{}\".format(int(self.fp16)))\n",
    "\n",
    "    if optim_states and saved_optim_type != opt['optimizer']:\n",
    "      print('WARNING: not loading optim state since optim class changed.')\n",
    "    elif optim_states:\n",
    "      optimstate_fp16 = 'loss_scaler' in optim_states\n",
    "      if self.fp16 and optimstate_fp16:\n",
    "        optim_states['loss_scaler'] = self.optimizer.state_dict()['loss_scaler']\n",
    "      elif optimstate_fp16 and not self.fp16:\n",
    "        optim_states = optim_states['optimizer_state_dict']\n",
    "      elif not optimstate_fp16 and self.fp16:\n",
    "        self.optimizer.optimizer.load_state_dict(optim_states)\n",
    "        return\n",
    "\n",
    "      # finally, try to actually load the optimizer state\n",
    "      try:\n",
    "        self.optimizer.load_state_dict(optim_states)\n",
    "      except ValueError:\n",
    "        print('WARNING: not loading optim state since model params changed.')\n",
    "\n",
    "  def build_lr_scheduler(self, states=None, hard_reset=False):\n",
    "    \"\"\"\n",
    "    Create the learning rate scheduler, and assign it to self.scheduler.\n",
    "    \"\"\"\n",
    "    # first make sure there are no null pointers\n",
    "    if states is None:\n",
    "      states = {}\n",
    "    optimizer = self.optimizer\n",
    "\n",
    "    warmup_updates = self.opt.get('warmup_updates', -1)\n",
    "    updates_so_far = states.get('number_training_updates', 0)\n",
    "    if warmup_updates > 0 and (updates_so_far < warmup_updates or hard_reset):\n",
    "\n",
    "      def _warmup_lr(step):\n",
    "        start = self.opt['warmup_rate']\n",
    "        end = 1.0\n",
    "        progress = min(1.0, step / self.opt['warmup_updates'])\n",
    "        lr_mult = start + (end - start) * progress\n",
    "        return lr_mult\n",
    "\n",
    "      self.warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, _warmup_lr)\n",
    "    else:\n",
    "      self.warmup_scheduler = None\n",
    "\n",
    "    patience = self.opt.get('lr_scheduler_patience', 3)\n",
    "    decay = self.opt.get('lr_scheduler_decay', 0.5)\n",
    "\n",
    "    if self.opt.get('lr_scheduler') == 'none':\n",
    "      self.scheduler = None\n",
    "    elif decay == 1.0:\n",
    "      warn_once(\n",
    "        \"Your LR decay is set to 1.0. Assuming you meant you wanted \"\n",
    "        \"to disable learning rate scheduling. Adjust --lr-scheduler-decay \"\n",
    "        \"if this is not correct.\"\n",
    "      )\n",
    "      self.scheduler = None\n",
    "    elif self.opt.get('lr_scheduler') == 'reduceonplateau':\n",
    "      self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', factor=decay, patience=patience, verbose=True\n",
    "      )\n",
    "    elif self.opt.get('lr_scheduler') == 'fixed':\n",
    "      self.scheduler = optim.lr_scheduler.StepLR(optimizer, patience, gamma=decay)\n",
    "    elif self.opt.get('lr_scheduler') == 'invsqrt':\n",
    "      if self.opt.get('warmup_updates', -1) <= 0:\n",
    "        raise ValueError('--lr-scheduler invsqrt requires setting --warmup-updates')\n",
    "      warmup_updates = self.opt['warmup_updates']\n",
    "      decay_factor = np.sqrt(max(1, warmup_updates))\n",
    "\n",
    "      def _invsqrt_lr(step):\n",
    "        return decay_factor / np.sqrt(max(1, step))\n",
    "\n",
    "      self.scheduler = optim.lr_scheduler.LambdaLR(optimizer, _invsqrt_lr)\n",
    "    else:\n",
    "      raise ValueError(\n",
    "        \"Don't know what to do with lr_scheduler '{}'\".format(self.opt.get('lr_scheduler'))\n",
    "      )\n",
    "\n",
    "    # time to load LR state from the checkpoint, if possible.\n",
    "    if (\n",
    "      # there is already an old LR scheduler saved on disk\n",
    "      states\n",
    "      and\n",
    "      # and the old LR scheduler is different\n",
    "      states.get('lr_scheduler_type') != self.opt['lr_scheduler']\n",
    "      and\n",
    "      # and we're not already using a fresh scheduler\n",
    "      not hard_reset\n",
    "    ):\n",
    "      # the LR scheduler changed, start things fresh\n",
    "      warn_once(\"LR scheduler is different from saved. Starting fresh!\")\n",
    "      hard_reset = True\n",
    "\n",
    "    if hard_reset:\n",
    "      # We're not going to use the LR schedule, let's just exit\n",
    "      return\n",
    "\n",
    "    # do the actual loading (if possible)\n",
    "    if 'number_training_updates' in states:\n",
    "      self._number_training_updates = states['number_training_updates']\n",
    "    if self.scheduler and 'lr_scheduler' in states:\n",
    "      self.scheduler.load_state_dict(states['lr_scheduler'])\n",
    "    if states.get('warmup_scheduler') and getattr(self, 'warmup_scheduler', None):\n",
    "      self.warmup_scheduler.load_state_dict(states['warmup_scheduler'])\n",
    "\n",
    "  def backward(self, loss):\n",
    "    \"\"\"\n",
    "    Perform a backward pass.\n",
    "    \"\"\"\n",
    "    update_freq = self.opt.get('update_freq', 1)\n",
    "    if update_freq > 1:\n",
    "      # gradient accumulation, but still need to average across the minibatches\n",
    "      loss = loss / update_freq\n",
    "      # we're doing gradient accumulation, so we don't only want to step\n",
    "      # every N updates instead\n",
    "      self._number_grad_accum = (self._number_grad_accum + 1) % update_freq\n",
    "\n",
    "    if self.fp16:\n",
    "      delay_unscale = update_freq > 1 and self._number_grad_accum > 0\n",
    "      with self.amp.scale_loss(loss, self.optimizer, delay_unscale=delay_unscale) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    else:\n",
    "      loss.backward()\n",
    "\n",
    "  def update_params(self):\n",
    "    \"\"\"\n",
    "    Perform step of optimization.\n",
    "    \"\"\"\n",
    "    update_freq = self.opt.get('update_freq', 1)\n",
    "    if update_freq > 1 and self._number_grad_accum > 0:\n",
    "      return\n",
    "\n",
    "    # keep track up number of steps, compute warmup factor\n",
    "    self._number_training_updates += 1\n",
    "\n",
    "    # compute warmup adjustment if needed\n",
    "    if self.opt.get('warmup_updates', -1) > 0:\n",
    "      if not hasattr(self, 'warmup_scheduler'):\n",
    "        raise RuntimeError('Looks like you forgot to call build_lr_scheduler')\n",
    "      if self._is_lr_warming_up():\n",
    "        self.warmup_scheduler.step(epoch=self._number_training_updates)\n",
    "\n",
    "    if self.opt.get('lr_scheduler') == 'invsqrt' and not self._is_lr_warming_up():\n",
    "      # training step scheduler\n",
    "      self.scheduler.step(self._number_training_updates)\n",
    "\n",
    "    if hasattr(opt, 'gradF'):\n",
    "      opt.gradF(self.model, self.getParameters())\n",
    "\n",
    "    if self.opt.get('gradient_clip', -1) > 0:\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(self.getParameters(), self.opt['gradient_clip'])\n",
    "      self.metrics['gnorm'] += grad_norm\n",
    "      self.metrics['clip'] += float(grad_norm > self.opt['gradient_clip'])\n",
    "\n",
    "    self.metrics['updates'] += 1\n",
    "    self.optimizer.step()\n",
    "    if hasattr(opt, 'paraF'):\n",
    "      opt.paraF(opt, self.model)\n",
    "\n",
    "  def train_step(self, batch):\n",
    "    \"\"\"Process batch of inputs and targets and train on them.\n",
    "\n",
    "    :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                  version of observations.\n",
    "    \"\"\"\n",
    "    if batch.text_vec is None:\n",
    "      return\n",
    "    self.is_training = True\n",
    "    self.model.train()\n",
    "    self.zero_grad()\n",
    "    output = self.model(batch.text_vec, batch.text_mask)\n",
    "    loss = self.loss(self, self.model, batch.label_vec, *output).sum()\n",
    "    if torch.allclose(loss, nan, equal_nan=True):\n",
    "      raise Exception('Loss returns NaN')\n",
    "    self.backward(loss)\n",
    "    self.update_params()\n",
    "    count = int(batch.text_lengths.sum())\n",
    "    self.metrics['count'] += count\n",
    "    self.metrics['loss.sum'] += float(loss) * count\n",
    "    return # omit response for speed\n",
    "    #pred = predict(output[0], batch.text_lengths)\n",
    "    #return Output(text=pred)\n",
    "\n",
    "  def eval_step(self, batch):\n",
    "    \"\"\"Process batch of inputs.\n",
    "\n",
    "    If the batch includes labels, calculate validation metrics as well.\n",
    "\n",
    "    :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                  version of observations.\n",
    "    \"\"\"\n",
    "    if batch.text_vec is None:\n",
    "      return\n",
    "    self.is_training = False\n",
    "    self.model.eval()\n",
    "    output = self.model(batch.text_vec, batch.text_mask)\n",
    "    if batch.label_vec is not None:\n",
    "      # Interactive mode won't have a gold label\n",
    "      missed = self.criterion(batch.label_vec, output[0], batch.text_mask)\n",
    "      self.metrics['error.sum'] += float(missed.sum())\n",
    "      self.metrics['eval_exs'] += int(batch.text_lengths.sum())\n",
    "\n",
    "    pred = predict(output[0], batch.text_lengths)\n",
    "    text = self._v2t(batch.text_vec[0])\n",
    "    self.vars = (text, pred[0], batch.text_vec[0], int(batch.text_lengths[0]), *tuple(v[0] for v in output[2:]))\n",
    "    return Output(text=pred)\n",
    "\n",
    "  def report(self):\n",
    "    \"\"\"Return metrics calculated by the model.\"\"\"\n",
    "    metrics = super().report()\n",
    "    if 'loss.sum' in self.metrics:\n",
    "      count = self.metrics['count'] if 'count' in self.metrics and self.metrics['count'] else 1\n",
    "      self.metrics['loss'] = self.metrics['loss.sum'] / count\n",
    "    metrics['loss'] = self.metrics['loss']\n",
    "    metrics['error'] = self.metrics['error.sum'] / (self.metrics['eval_exs'] if self.metrics['eval_exs'] else 1)\n",
    "    metrics['accuracy'] = 1. - metrics['error']\n",
    "    if self.writeVars:\n",
    "      self.writeVars({'loss': metrics['loss']},\n",
    "        histograms=dict(self.model.named_parameters()),\n",
    "        n=self.scheduler.last_epoch)\n",
    "    if len(self.vars):\n",
    "      if self.drawVars:\n",
    "        self.drawVars(*self.vars[2:])\n",
    "      if len(self.vars) > 1 and self.writeVars:\n",
    "        self.writeVars(images={'hidden': self.vars[4].unqueeze(1)},\n",
    "          n=self.scheduler.last_epoch)\n",
    "      if type(self.vars[0]) == str:\n",
    "        print(self.vars[0], self.vars[1])\n",
    "      self.vars = []\n",
    "    return metrics\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    \"\"\"Reset metrics calculated by the model back to zero.\"\"\"\n",
    "    super().reset_metrics()\n",
    "    self.metrics['loss'] = 0.\n",
    "    self.metrics['loss.sum'] = 0.\n",
    "    self.metrics['error.sum'] = 0.\n",
    "    self.metrics['count'] = 0\n",
    "    self.metrics['eval_exs'] = 0\n",
    "\n",
    "  def receive_metrics(self, metrics_dict):\n",
    "    \"\"\"Update lr scheduler with validation loss.\"\"\"\n",
    "    return super().receive_metrics(metrics_dict)\n",
    "\n",
    "  @classmethod\n",
    "  def add_cmdline_args(cls, argparser):\n",
    "    \"\"\"Add command-line arguments specifically for this agent.\"\"\"\n",
    "    super(MyAgent, cls).add_cmdline_args(argparser)\n",
    "\n",
    "    agent = argparser.add_argument_group('Arguments')\n",
    "    agent.add_argument('-esz', '--embeddingsize', type=int, default=16,\n",
    "                        help='size of the token embeddings')\n",
    "    agent.add_argument('-dr', '--dropout', type=float, default=0.0,\n",
    "                        help='dropout rate')\n",
    "    agent.add_argument('--fp16', type=_isFp16, default=2,\n",
    "                        help='Amp fp16 optimization level')\n",
    "    argparser.set_defaults(split_lines=True)\n",
    "    MyAgent.dictionary_class().add_cmdline_args(argparser)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see one data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.tasks.babi.agents import Task1kTeacher\n",
    "teacher = Task1kTeacher(dict(task='babi:Task1k:0', datapath='/usr/local/anaconda3/lib/python3.6/site-packages/parlai-0.1.0-py3.6.egg/data', datatype='train'))\n",
    "teacher.next_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with ParlAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GIT_PYTHON_GIT_EXECUTABLE'] = '/usr/bin/git'\n",
    "import parlai.scripts.train_model as train\n",
    "from parlai.scripts.train_model import TrainLoop\n",
    "from my_agent import MyAgent as ModelClass\n",
    "parser = train.setup_args()\n",
    "parser.add_argument('-f', type=str, default='', help='')\n",
    "ModelClass.add_cmdline_args(parser)\n",
    "opt = parser.parse_args()\n",
    "opt.update({\n",
    "  'model_file': 'myModel',\n",
    "  'task': 'integration_tests',\n",
    "  'dict_file': 'integrationTests.dict',\n",
    "  'dict_lower': True,\n",
    "  'history_size': 1,\n",
    "  'numthreads': 2,\n",
    "  'num_epochs': 3,\n",
    "  'batchsize': 16,\n",
    "  'optimizer': 'sgd',\n",
    "  'learningrate': .1,\n",
    "  'momentum': .9,\n",
    "  'save_after_valid': True,\n",
    "  'validation_every_n_epochs': 1,\n",
    "  'metrics': 'correct,accuracy',\n",
    "  'log_every_n_secs': 30\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ModelClass(opt)\n",
    "train.create_agent = lambda *_: agent\n",
    "trainLoop = train.TrainLoop(opt)\n",
    "res = trainLoop.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}